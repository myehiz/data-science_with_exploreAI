{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regression MCQ\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this MCQ, we will engage in a comprehensive multiple-choice exercise, applying regression concepts and techniques to agricultural yield prediction. Through a series of challenges, we'll analyse variable relationships, feature engineering, model construction, and evaluation, enhancing our understanding and proficiency in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "* Analyse predictor variables and their relationship with the target variable.\n",
    "* Perform feature engineering tasks, including encoding categorical variables and scaling features.\n",
    "* Construct and evaluate multiple linear regression models using appropriate libraries.\n",
    "* Identify and address multicollinearity issues using regularisation techniques such as LASSO and Ridge regression.\n",
    "* Interpret regression coefficients and understand their impact on the target variable.\n",
    "* Implement decision tree models for prediction tasks, exploring both categorical and numerical data\n",
    "* Calculate and interpret MSE and RMSE for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The data\n",
    "Join us on an intriguing exploration of a vast agricultural dataset. Uncover the intricate connections between geographic, weather, soil, and farm management features, all leading to a prediction of yield. Our mission? To decode these relationships and empower farmers with actionable insights for improved productivity and informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary\n",
    "\n",
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield (DUMMY VARIABLE - Removed)\n",
    "\n",
    "<br>\n",
    "\n",
    "**5. Target variable**\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our database again, like we did last time. We won't use the weather data so it is commented out.\n",
    "\n",
    "**Important:** Ensure that `data_ingestion.py` file and the `field_data_processor.py` files are stored in the same folder as your notebook, otherwise the data import will fail.\n",
    " \n",
    "[Download files here](https://github.com/Explore-AI/Public-Data/raw/master/Maji_Ndogo/modules.zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.graphics.correlation as sgc\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database, and clean the data using the processing modules we built.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "# from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "    \"regex_patterns\" : {\n",
    "            'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "            'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "            'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "            },\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# We're not going to use the weather data this time, so we'll ignore it.\n",
    "# weather_processor = WeatherDataProcessor(config_params)\n",
    "# weather_processor.process()\n",
    "# weather_df = weather_processor.weather_df\n",
    "\n",
    "dataset = field_df.drop(\"Weather_station\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into our analysis, it's crucial to ensure the integrity of our dataset and that the data is still as we expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data\n",
    "# !pip install pytest\n",
    "\n",
    "dataset.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Understanding our variables and variable selection\n",
    "In this section, we will analyse the predictor variables in our dataset and how they relate to the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many predictors do we originally have in our dataset, and which of these are categorical in nature?\n",
    "\n",
    "**Hint**: The `Field_ID` serves as a unique identifier for each field and does not provide any predictive value for modeling.\n",
    "\n",
    "#### Options\n",
    "* 16 predictors; Location, Soil_type, and Crop_type are categorical.\n",
    "* 18 predictors; Location, Soil_type, and Crop_type are categorical.\n",
    "* 17 predictors; Location, Soil_type, and Crop_type are categorical.\n",
    "* 16 predictors; Location, Soil_type, and Plot_size are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# dataset.head()\n",
    "output=len(dataset.columns)- 2\n",
    "print(f\"{output} predictors;\")\n",
    "\n",
    "dataset.select_dtypes(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "The categorical features in our dataset need to be converted into a format suitable for modeling. After applying dummy variable encoding to these categorical features, how many independent variables do we now have?\n",
    "\n",
    "**Note**: Set the `drop_first` parameter to True.\n",
    "\n",
    "**Hint**: Remember to exclude `Field_ID` \n",
    "\n",
    "#### Options\n",
    "* 31\n",
    "* 29\n",
    "* 17\n",
    "* 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "dataset_encoded=pd.get_dummies(dataset,columns=['Location', 'Soil_type', 'Crop_type'],drop_first=True)\n",
    "# dataset_encoded\n",
    "count_independent_variables=dataset_encoded.shape[1] - 2\n",
    "print(count_independent_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "From our encoded dataset, which variable has the third highest absolute correlation with the `Standard_yield`, excluding the variable itself?\n",
    "\n",
    "#### Options\n",
    "* Pollution_level\n",
    "* Crop_type_tea\n",
    "* Annual_yield \n",
    "* Longitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "correlation_matrix=dataset_encoded.corr()\n",
    "Correlation_wsy=correlation_matrix['Standard_yield']\n",
    "abs_Correlation_wsy=Correlation_wsy.abs()\n",
    "sort_abs_Correlation_wsy=abs_Correlation_wsy.sort_values(ascending=False)\n",
    "sort_abs_Correlation_wsy=sort_abs_Correlation_wsy.drop('Standard_yield')\n",
    "third_highest_corr= sort_abs_Correlation_wsy.index[2]\n",
    "print(third_highest_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "In order to fit an ordinary least squares regression model to our encoded data we would need to make sure that all our variables are numeric. Through dummy variable encoding, we converted our categorical variables to multiple int/boolean variables (depending on the environment you are working on). The `sm.OLS()` method itself does not inherently handle boolean data types implicitly.\n",
    "\n",
    "In most cases, when fitting a model with sm.OLS(), it's essential to ensure that all features are numeric. If boolean columns are present, they should be explicitly converted to numeric types before fitting the model.\n",
    "\n",
    "Suppose our columns converted to boolean datatypes, which of the following statements is true after converting the boolean columns in the dataset to integer datatype?\n",
    "\n",
    "* The boolean columns have been replaced with binary integer representations, with True converted to 1 and False converted to 0.\n",
    "* The boolean columns have been removed from the dataset.\n",
    "* The boolean columns have been replaced with binary integer representations, with True converted to 0 and False converted to 1.\n",
    "* The boolean columns have been converted to a unique string datatype that is also an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "bool_columns=dataset_encoded.select_dtypes(include=[bool]).columns\n",
    "dataset_encoded[bool_columns]=dataset_encoded[bool_columns].astype(int)\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Suppose we wish to determine which of the predictors in our dataset are statistically significant. We will follow the following steps:\n",
    "\n",
    "* Fit a statsmodels regression model to predict `Standard_yield` using our encoded dataset.\n",
    "* Extract p-values for each predictor from the fitted model to find the ones that are significant based on a given threshold.\n",
    "\n",
    "Which of the following variables do we find to be statistically significant for predicting `Standard_yield`, based on a p-value threshold of 0.05 `(p-value < 0.05)`?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* Slope\n",
    "* Elevation\n",
    "* pH\n",
    "* Soil_fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import statsmodels.api as sm\n",
    "X= dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y=dataset_encoded['Standard_yield']\n",
    "\n",
    "model=sm.OLS(y,X).fit()\n",
    "\n",
    "p_values=model.pvalues\n",
    "\n",
    "significant_pred=p_values[p_values < 0.05]\n",
    "significant_pred=round(significant_pred.sort_values(ascending=False),7)\n",
    "print(significant_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Generating a multiple linear regression model\n",
    "\n",
    "After encoding our variables and performing preliminary analysis, let's continue our efforts to identify the variables that will enable us to construct the most accurate multiple linear regression model possible with the data that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Create a correlation matrix using all the columns in our encoded dataset and display it as a heatmap.<br>\n",
    "\n",
    "Which of the following independent variables have a strong correlation with one another.\n",
    "\n",
    "* Plot_size and Crop_type_tea\n",
    "* Pollution_level and Soil_type_Rocky\n",
    "* Crop_type_cassava and Soil_type_Sandy\n",
    "* Min_temperature_C and Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation_matrix\n",
    "Correlation_matrix=dataset_encoded.corr()\n",
    "\n",
    "plt.figure(figsize=(22,20))\n",
    "sns.heatmap(Correlation_matrix,annot=True,cmap='coolwarm',fmt=\".2f\")\n",
    "plt.title('Correlation_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations= Correlation_matrix.loc[[\"Plot_size\",\"Pollution_level\",\"Crop_type_cassava\",\"Min_temperature_C\"],[\"Crop_type_tea\",\"Soil_type_Rocky\",\"Soil_type_Sandy\",\"Elevation\"]]\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Now that we have analysed our variables, let's fit an ordinary least squares regression model using `statsmodels.formula.api` and then print the model summary. Construct the model using all the independent variables in our encoded dataset (excuding `Field_ID`).<br>\n",
    "\n",
    "Which of the following statements accurately describes the interpretation of the F-statistic in the context of our regression model?\n",
    "\n",
    "#### Options\n",
    "* The F-statistic tests the overall significance of the regression model. A low F-statistic value with a corresponding high p-value  indicates that the regression model is statistically significant, meaning that at least one of the independent variables has a significant effect on the dependent variable.\n",
    "* The F-statistic tests the overall significance of the regression model. A high F-statistic value with a corresponding low p-value indicates that the regression model is statistically significant, meaning that at least one of the independent variables has a significant effect on the dependent variable.\n",
    "* The F-statistic tests the overall significance of the regression model. A high F-statistic value with a corresponding high p-value indicates that the regression model is statistically significant, meaning that at least one of the independent variables has a significant effect on the dependent variable.\n",
    "* The F-statistic tests the overall significance of the regression model. A low F-statistic value with a corresponding low p-value  indicates that the regression model is not statistically significant, meaning that none of the independent variables have a significant effect on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "variables=\"Standard_yield ~\"+\"+\".join([col for col in dataset_encoded.columns if col!=\"Field_ID\" and col!=\"Standard_yield\"])\n",
    "\n",
    "model=smf.ols(formula=variables,data=dataset_encoded).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 \n",
    "\n",
    "This summary  gives us an indication of possible multicollinearity present within our predictor variables. The presence of any correlation among predictors is detrimental to model quality because it tends to increase the standard error of the coefficients and it becomes difficult to estimate the effect of any one predictor variable on the response variable.\n",
    "\n",
    "To avoid this, let's reduce the number of independent variables included in our model.\n",
    "Fit the model using the following variables:<br>\n",
    "\n",
    "* `Pollution_level` \n",
    "* `Crop_type_coffee` \n",
    "* `Crop_type_tea`\n",
    "* `Location_Rural_Sokoto`\n",
    "* `Annual_yield` \n",
    "* `Soil_type_Silt`\n",
    "* `Soil_type_Volcanic`\n",
    "\n",
    "After reducing the number of dependent variables accordingly, how did the model change?\n",
    "\n",
    "#### Options:\n",
    "\n",
    "* The model worsened.\n",
    "* The model improved.\n",
    "* The model remained unchanged.\n",
    "* It cannot be determined from the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "variables=\"Standard_yield ~ Pollution_level + Crop_type_coffee + Crop_type_tea + Location_Rural_Sokoto + Annual_yield + Soil_type_Silt+ Soil_type_Volcanic\"\n",
    "\n",
    "model=smf.ols(formula=variables,data=dataset_encoded).fit()\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Let's evaluate our model's results. Generate a scatter plot of the residuals against the fitted values allowing us to visually inspect whether the residuals have constant variance and are distributed randomly around the zero residual line.\n",
    "\n",
    "What does the scatter plot tell us?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* The model has perfect predictive accuracy.\n",
    "* The plot indicates homoscedasticity as residuals have a constant variance and are distributed randomly around the zero residual line.\n",
    "* There is no linear relationship between the dependent and independent variables.\n",
    "* The model suffers from multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "fitted_values=model.fittedvalues\n",
    "residuals=model.resid\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(fitted_values,residuals)\n",
    "plt.axhline(y=0,color='r',linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "If a linear regression model indicated heteroscedasticity, which of the following actions could be considered to address this issue?\n",
    "\n",
    "* Applying transformations to the independent variables to better fit the linear relationship.\n",
    "* Implementing weighted least squares regression to give less emphasis to observations with higher variance in residuals.\n",
    "* Removing outliers from the dataset to reduce the impact of extreme values on the variance of residuals.\n",
    "* All of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Using regularisation to optimise agricultural yield\n",
    "\n",
    "Continuing with our task, after fitting our model, we decide to go back to the drawing board to explore more ways to implement feature engineering and data pre-processing in order to optimise our model. We suspect that, if we use all the variables available to us, the model might overfit due to the high dimensionality of the data. Regularisation techniques will therefore be critical in building a predictive model that generalises well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Our first step in the feature engineering involves creating a new feature, `Temperature_Range`, and scaling features using `StandardScaler`. The difference between `Min_temperature_C` and `Max_temperature_C` is the temperature range.\n",
    "\n",
    "Given the code block below, which option correctly completes the feature engineering step of creating the `Temperature_Range` column?\n",
    "\n",
    "**Hint**: Insert the correct code snippet into the appropriate location within the code block to run it.\n",
    "\n",
    "#### Options:\n",
    "\n",
    "* `dataset['Temperature_Range'] = dataset[['Min_temperature_C','Max_temperature_C']].min(axis=1)`\n",
    "* `dataset['Temperature_Range'] = dataset['Max_temperature_C'] - dataset['Min_temperature_C']`\n",
    "* `dataset = dataset.assign(Temperature_Range = dataset['Max_temperature'] - dataset['Min_temperature'])`\n",
    "* `dataset[['Temperature_Range']] = dataset[['Max_temperature_C']] - dataset[['Min_temperature_C']]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Insert selected option here\n",
    "\n",
    "dataset['Temperature_Range'] = dataset['Max_temperature_C'] - dataset['Min_temperature_C']\n",
    "\n",
    "# Select features for scaling (exclude non-numeric or target variables)\n",
    "features = ['Elevation', 'Slope', 'Rainfall', 'Ave_temps', 'Temperature_Range', 'Soil_fertility', 'pH', 'Pollution_level']\n",
    "\n",
    "# Initialise StandardScaler and apply it to the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(dataset[features])\n",
    "\n",
    "# Show the first 5 rows of the scaled features\n",
    "print(scaled_features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "\n",
    "Consider a scenario where we decide to employ LASSO regression to identify predictive features associated with our dependent variable.\n",
    "\n",
    "We implement the Python code block provided below.\n",
    "\n",
    "What is the purpose of the `LassoCV(cv=5)` constructor parameter `cv=5`?\n",
    "\n",
    "#### Options:\n",
    "\n",
    "* It indicates that 5-fold cross-validation should be used to select the best regularisation parameter.\n",
    "* It sets the penalty coefficient to 5, increasing the regularisation strength.\n",
    "* It specifies that the LASSO model should use a 5-degree polynomial feature expansion.\n",
    "* It defines that the model should only select 5 features at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# scaled_features is our matrix of scaled features and dataset['Standard_yield'] is the target variable\n",
    "lasso = LassoCV(cv=5).fit(scaled_features, dataset['Standard_yield'])\n",
    "\n",
    "# Find the features with non-zero coefficients\n",
    "selected_features = [features[i] for i, coef in enumerate(lasso.coef_) if coef != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "In trying to address multicollinearity in our dataset, we also decide to implement Ridge regression. After understanding that Ridge regression applies an L2 penalty to the coefficients to reduce their magnitude without setting them to zero, we decide to use `RidgeCV` for applying Ridge regression with cross-validation to select the optimal penalty strength. Given the snippet of code below, which parameter correctly adjusts the strength of the regularisation applied to the model?\n",
    "\n",
    "**Hint**: Insert the correct code snippet into the appropriate location within the code block to run it.\n",
    "\n",
    "#### Options:\n",
    "* `cv=[0.001, 0.01, 0.1, 1, 10, 100]`\n",
    "* `fit_intercept=True`\n",
    "* `alphas=[0.001, 0.01, 0.1, 1, 10, 100]`\n",
    "* `scoring='neg_mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Insert selected option here\n",
    "\n",
    "alphas=[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "# Apply Ridge regression with cross-validation\n",
    "ridge = RidgeCV(alphas=alphas, cv=5).fit(scaled_features, dataset['Standard_yield'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Given our dataset includes variables such as `Elevation` and `Slope`, and considering the potential interactions between these variables might impact crop yield, we aim to capture both these interactions and possible non-linear relationships. Which of the following methods is specifically designed to create a quadratic interaction term without including an intercept in the feature set?\n",
    "\n",
    "**HINT:** Consider utilising `sklearn`'s feature transformation tools for this purpose.\n",
    "\n",
    "* `poly = PolynomialFeatures(degree=2, include_bias=False)` \n",
    "* `poly = PolynomialFeatures(degree=1, include_bias=False)` \n",
    "* `poly = PolynomialFeatures(degree=3, include_bias=True)` \n",
    "* `poly = PolynomialFeatures(degree=2, include_bias=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate polynomial and interaction features\n",
    "#Insert selected option here\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "poly_features = poly.fit_transform(dataset[['Elevation', 'Slope']])\n",
    "\n",
    "# Display the shape of the new feature matrix\n",
    "poly_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "After implementing Ridge regression to address multicollinearity and prevent overfitting in our model we need to interpret the coefficients to understand the impact of each feature on the standard yield.\n",
    "\n",
    "Based on the output of the Ridge regression coefficients in the code block below, which statement is true regarding the impact of each feature on the standard yield?\n",
    "\n",
    "#### Options:\n",
    "\n",
    "* Feature 1 has the highest positive impact, followed by Feature 2 and Feature 3.\n",
    "* Feature 3 has the highest positive impact, followed by Feature 1 and Feature 2.\n",
    "* The coefficients are inconclusive; further analysis is needed to determine the impact of each feature.\n",
    "* Feature 2 has the highest positive impact, while Feature 3 and Feature 1 have negative impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `X` is the feature matrix and `y` is the target variable\n",
    "X = np.array([[0.5, 0.2, 0.1],\n",
    "              [0.9, 0.3, 0.5],\n",
    "              [0.3, 0.8, 0.2]])\n",
    "y = np.array([0.7, 0.6, 0.8])\n",
    "\n",
    "# Fit Ridge regression model with alpha = 0.1\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "# Display the coefficients of the model\n",
    "ridge_model.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Making a prediction using decision trees\n",
    "\n",
    "After learning that decision trees are easy to implement and are capable of handling both categorical and numerical data while being resilient to outliers, we decide to implement a decision tree on our encoded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "Train a decision tree with the following specifications:\n",
    "* Using our **previously encoded dataset**, split the data into dependent and independent variables using all the features except for `Standard_yield` and `Field_ID` as independent variables.\n",
    "* Split the data into training and testing data.\n",
    "* Use the `DecisionTreeRegressor` to fit a model using a `max_depth' of 2 and a `random_state` of 42.\n",
    "\n",
    "Using the trained Decision Tree Regressor model, make a prediction for `y` given the following x-values:<br> \n",
    "`[864.66138, -8.12890218821531, -8.311822719284072, 16.274624300000003, 1237.7200000000003, -3.4100000000000006, 36.410000000000004,`\n",
    "`16.5,0.682, 6.7863323423108195, 0.09379352739936421, 1.4300000000000002, 0.8264890400277934,0.0,0.0,0.0,0.0,0.0,0.0,1.1,0.0,0.0,1.1,0.0, 0.0,0.0,0.0,0.0,0.0]`\n",
    "\n",
    "What is the value of the predicted y?\n",
    "\n",
    "#### Options\n",
    "* 0.3250077\n",
    "* 0.6654377\n",
    "* 0.48494414\n",
    "* 0.8050340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y= dataset_encoded['Standard_yield']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "dec_tree_reg_model=DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "\n",
    "dec_tree_reg_model.fit(X_train,y_train)\n",
    "\n",
    "X_values=[[864.66138, -8.12890218821531, -8.311822719284072, 16.274624300000003, 1237.7200000000003, -3.4100000000000006, 36.410000000000004, 16.5,0.682, 6.7863323423108195, 0.09379352739936421, 1.4300000000000002, 0.8264890400277934,0.0,0.0,0.0,0.0,0.0,0.0,1.1,0.0,0.0,1.1,0.0, 0.0,0.0,0.0,0.0,0.0]]\n",
    "\n",
    "predicted_y= dec_tree_reg_model.predict(X_values)\n",
    "predicted_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17\n",
    "\n",
    "Based on the model above, what is the value of our RMSE?\n",
    "\n",
    "* 0.0658\n",
    "* 0.0881\n",
    "* 0.5656\n",
    "* 0.8810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "predicted_y= dec_tree_reg_model.predict(X_test)\n",
    "mse=mean_squared_error(y_test,predicted_y)\n",
    "RMSE=np.sqrt(mse)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18\n",
    "Which of the following statements is correct about our RMSE?\n",
    "\n",
    "#### Options\n",
    "* An RMSE of 0.0881 suggests that, on average, the predicted values are off by approximately 0.0881 units\n",
    "* An RMSE value of 0.0881 suggests that, at most, the predicted values deviate by approximately 0.08 units from the actual values.\n",
    "* An RMSE of 0.0881 indicates a perfect fit of the model to the training data.\n",
    "* An RMSE of 0.0881 suggests that the predicted values are correct 8.81% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19 \n",
    "What is the likely effect of adjusting the `max_depth` parameter in a Decision Tree model?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* Lower max_depth values may lead to increased model complexity and a higher risk of underfitting\n",
    "* Higher max_depth values may lead to decreased model complexity and a lower risk of overfitting\n",
    "* Higher max_depth values reduce the likelihood of capturing complex relationships in the data.\n",
    "* Higher max_depth values may lead to increased model complexity and a higher risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "Higher max_depth values may lead to increased model complexity and a higher risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20 \n",
    "Let's attempt to enhance our model's performance by setting the `max_depth` hyperparameter to 5.\n",
    "\n",
    "True or false? The decision tree model was improved by fitting it with a `max_depth` parameter of 5.\n",
    "\n",
    "#### Options\n",
    "* True\n",
    "* False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X = dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y= dataset_encoded['Standard_yield']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "dec_tree_reg_model=DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "dec_tree_reg_model.fit(X_train,y_train)\n",
    "\n",
    "X_values=[[864.66138, -8.12890218821531, -8.311822719284072, 16.274624300000003, 1237.7200000000003, -3.4100000000000006, 36.410000000000004, 16.5,0.682, 6.7863323423108195, 0.09379352739936421, 1.4300000000000002, 0.8264890400277934,0.0,0.0,0.0,0.0,0.0,0.0,1.1,0.0,0.0,1.1,0.0, 0.0,0.0,0.0,0.0,0.0]]\n",
    "\n",
    "predicted_y= dec_tree_reg_model.predict(X_values)\n",
    "predicted_y\n",
    "\n",
    "# predicted_y= dec_tree_reg_model.predict(X_test)\n",
    "# mse=mean_squared_error(y_test,predicted_y)\n",
    "# RMSE=np.sqrt(mse)\n",
    "# RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_y= dec_tree_reg_model.predict(X_test)\n",
    "mse=mean_squared_error(y_test,predicted_y)\n",
    "RMSE=np.sqrt(mse)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Ensemble Methods & Bootstrapping\n",
    "\n",
    "Using our original dataset, our objective is to explore ensemble methods and bootstrapping techniques to enhance the model performance. We'll work with a subset of features from the dataset to predict the `Standard_yield`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21 \n",
    "Consider the following approach to implement a bootstrap aggregation (bagging) for predicting the `Standard_yield` based on features `Elevation`, `Slope`, `Soil_fertility`, and `Pollution_level`. Given are the steps and part of the Python code implementing this method. Our task is to identify the correct piece of code that completes the implementation.\n",
    "\n",
    "The steps for the implementation are as follows:\n",
    "\n",
    "1. Create an empty list named `predictions` to store predictions from each bootstrap sample.\n",
    "2. Generate `n_bootstrap_samples` bootstrap samples from the original dataset.\n",
    "3. For each bootstrap sample, fit a linear regression model and predict the `Standard_yield` on the entire dataset.\n",
    "4. Store each set of predictions in the predictions list.\n",
    "5. Average the predictions across all bootstrap samples to obtain the final bagged prediction.\n",
    "6. Compute and print the mean squared error (MSE) to evaluate the performance of the bagged model.\n",
    "\n",
    "\n",
    "Below is the code block with a missing part that needs to be completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# X_train, y_train represent the features and target variable from the training data\n",
    "X = dataset[['Elevation', 'Slope', 'Soil_fertility', 'Pollution_level']]\n",
    "y = dataset['Standard_yield']\n",
    "\n",
    "predictions = []\n",
    "n_bootstrap_samples = 100\n",
    "\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    X_sample, y_sample = resample(X, y)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_sample, y_sample)\n",
    "    y_pred = model.predict(X)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# MISSING PART HERE\n",
    "\n",
    "bagged_prediction = np.mean(predictions, axis=0)\n",
    "\n",
    "mse_bagged = mean_squared_error(y, bagged_prediction)\n",
    "\n",
    "print(f\"Mean Squared Error of Bagged Linear Regression Models: {mse_bagged}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which code should replace the # MISSING PART HERE section to correctly implement the averaging of predictions and computation of the mean squared error?\n",
    "\n",
    "#### Options\n",
    "\n",
    "\n",
    "* `bagged_prediction = np.average(predictions, axis=1)`\n",
    "\n",
    "   `mse_bagged = mean_squared_error(y, bagged_prediction)`\n",
    "<br><br>\n",
    "\n",
    "* `bagged_prediction = np.median(predictions, axis=0)`\n",
    "\n",
    "  `mse_bagged = mean_squared_error(y, bagged_prediction)`\n",
    "<br><br>\n",
    "\n",
    "* `bagged_prediction = np.mean(predictions, axis=0)`\n",
    "\n",
    "   `mse_bagged = mean_squared_error(y, bagged_prediction)`\n",
    "<br><br>\n",
    "\n",
    "* `bagged_prediction = np.mean(predictions, axis=1)`\n",
    "\n",
    "   `mse_bagged = mean_squared_error(y, bagged_prediction)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22 \n",
    "\n",
    "Given the following code snippet that applies a `RandomForestRegressor` to a dataset, which parameter in the `RandomForestRegressor` constructor is crucial for implementing the random subspace method by allowing the algorithm to select a random subset of features for each split?\n",
    "\n",
    "#### Options\n",
    "* `max_depth`\n",
    "* `random_state`\n",
    "* `max_features`\n",
    "* `n_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialise and train the random forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Predict using the random forest model\n",
    "y_pred_rf = rf_model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y, y_pred_rf)\n",
    "print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23\n",
    "\n",
    "### Question 23 (Medium)\n",
    "\n",
    "Consider the theoretical setup for a stacking ensemble model designed for a regression task. The first layer of this model includes three different types of regression models: linear regression, ridge regression, and a support vector machine (SVM) with a linear kernel. The second layer, or the final estimator, uses a linear regression model to combine the predictions from the first layer. The goal is to theoretically predict `Standard_yield` based on features such as `Elevation`, `Slope`, `Soil_fertility`, and `Pollution_level`, with the intention to evaluate the model's hypothetical performance using the Mean Squared Error (MSE).\n",
    "\n",
    "Given the following theoretical code snippet that outlines this stacking ensemble model's setup, what should replace the `_____` in the code to correctly configure the SVM with a linear kernel as part of the base learners in the stacking model?\n",
    "\n",
    "#### Options\n",
    "* `linear`\n",
    "* `lin`\n",
    "* `sigmoid`\n",
    "* `degree=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define base learners\n",
    "estimators = [\n",
    "    ('lr', LinearRegression()),\n",
    "    ('ridge', Ridge()),\n",
    "    ('svr', SVR(kernel='_____'))\n",
    "]\n",
    "\n",
    "# Define the theoretical stacking model\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "\n",
    "# Note: Assume X, y represent the features and target variable respectively, for a theoretical prediction scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24\n",
    "\n",
    "Consider the following Python code snippet that aims to implement a 5-fold cross-validation scheme to estimate the accuracy of a ridge regression model. This model uses bootstrapped samples within each fold to predict `Standard_yield` and calculates the average Mean Squared Error (MSE) across all folds.\n",
    "\n",
    "Which of the following options correctly fill in the blanks to ensure the code correctly implements the described functionality?\n",
    "\n",
    "#### Options\n",
    "* `scoring='r2'`, `cv=10`\n",
    "* `scoring='mean_absolute_error'`, `cv=5`\n",
    "* `scoring='neg_mean_squared_error'`, `cv=5`\n",
    "* `scoring='neg_mean_squared_error'`, `cv='Bootstrap'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialise the ridge regression model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Perform 5-fold cross-validation with bootstrapping\n",
    "scores = cross_val_score(ridge_model, X, y, scoring='neg_mean_squared_error', cv=5) #add correct option here \n",
    "\n",
    "# Convert scores to positive MSE\n",
    "mse_scores = -scores\n",
    "\n",
    "# Calculate average MSE\n",
    "average_mse = np.mean(mse_scores)\n",
    "print(f\"Average Mean Squared Error from Cross-Validation: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25\n",
    "\n",
    "Consider the code snippet that extracts and prints the feature importances from a trained random forest regressor model. The model is used to predict `Standard_yield` based on various features. The code utilises the `feature_importances_` attribute of the random forest model to obtain importance scores for each feature.\n",
    "\n",
    "Which of the following statements best describes the purpose and outcome of the provided code snippet?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* The code identifies and prints the importance scores for each feature in the random forest model, indicating how much each feature contributes to the model's ability to predict `Standard_yield`. Higher scores suggest a greater contribution to the prediction.\n",
    "\n",
    "* The code counts the number of times each feature is used to split the data across all trees in the random forest, thereby determining each feature's importance in predicting `Standard_yield`.\n",
    "\n",
    "* The code calculates and prints the coefficient values for each feature used in the  random forest, model to predict `Standard_yield`, indicating the strength and direction of the relationship between each feature and the target variable.\n",
    "\n",
    "* The code computes the correlation between each feature and the target variable `Standard_yield`, printing a list of features sorted by their correlation coefficients to identify the most relevant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Print feature importances\n",
    "for feature, importance in zip(X.columns, feature_importances):\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: Random forests\n",
    "In this challenge, we want to test how our data fits to a random forest model and other functionalities that come with its such as analysing feature importance.\n",
    "\n",
    "We are required to write a function named `train_rf_model` that trains and tests a random forest model on a given dataset. Our function should do the following:\n",
    "\n",
    "* Take a `RandomForestRegressor` object (with any desired hyperparameters set) as input,\n",
    "* Separate the features `X` and target `y` dataframes\n",
    "* Split the data into training and testing sets - use a test size of `20%` and a random state of `42` for reproducibility\n",
    "* Fit the model to the training data\n",
    "* Make predictions on the testing set\n",
    "* Return the trained model, the R-squared score, and the Mean Squared Error (MSE) of the test set predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 26\n",
    "1) Implement the function outlined above.\n",
    "\n",
    "2) Using the function, train a random forest model on our dataset with random_state set to 42, and max_depth=15, while leaving all other hyperparameters at their defaults. Use all the features available in the encoded dataset for this task. What are the R-squared and MSE scores for the model on the test data?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* R2: 0.0059, MSE: 0.6198\n",
    "* R2: 0.5555, MSE: 0.2345\n",
    "* R2: 0.9586, MSE: 0.0006\n",
    "* R2: 0.8196, MSE: 0.0500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from sklearn.metrics import r2_score\n",
    "def train_rf_model(rf_model,X,y):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    rf_model.fit(X_train,y_train)\n",
    "    \n",
    "    predictions= rf_model.predict(X_test)\n",
    "    \n",
    "    r_squaredscore=r2_score(y_test,predictions)\n",
    "    \n",
    "    mse=mean_squared_error(y_test,predictions)\n",
    "    \n",
    "    return  rf_model, r_squaredscore, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=RandomForestRegressor(random_state=42,max_depth=15)\n",
    "X = dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y= dataset_encoded['Standard_yield']\n",
    "rf_model, r_squaredscore, mse= train_rf_model(rf_model,X,y)\n",
    "print(r_squaredscore,mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 27\n",
    "\n",
    "We want to examine how our data will fit to a random forest model when we tune the number of trees. We want to train and compare two random forest models with the same dataset as in the previous exercise. The first model should be trained with `150` trees, and the second model with `200` trees. Both models should use the default hyperparameters for all other settings, apart from a random_state of `42` to ensure reproducibility. After evaluating both models on the test set, how does the error differ between the two models?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* The error increased when the number of trees was increased from 150 to 200.\n",
    "* The model with 200 trees showed a significant decrease in error compared to the model with 150 trees.\n",
    "* The model with 200 trees showed a very slight decrease in error compared to the model with 150 trees.\n",
    "* There was no change in the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "rf_model=RandomForestRegressor(n_estimators=150,random_state=42,max_depth=15)\n",
    "X = dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y= dataset_encoded['Standard_yield']\n",
    "rf_model, r_squaredscore, mse= train_rf_model(rf_model,X,y)\n",
    "print(r_squaredscore,mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=RandomForestRegressor(n_estimators=200,random_state=42,max_depth=15)\n",
    "X = dataset_encoded.drop(columns=['Standard_yield','Field_ID'])\n",
    "y= dataset_encoded['Standard_yield']\n",
    "rf_model, r_squaredscore, mse= train_rf_model(rf_model,X,y)\n",
    "print(r_squaredscore,mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 28\n",
    "Which of the following is a possible effect of increasing the number of trees in a random forest regression model?\n",
    "\n",
    "#### Options\n",
    "* Increasing the number of trees increases the modelâs predictive ability up to a certain point, after which additional trees do not significantly impact performance.\n",
    "* Increasing the number of trees always improves the model's predictive ability\n",
    "* Increasing the number of trees significantly decreases the model's predictive ability\n",
    "* The number of trees in a random forest model has no impact on the model's predictive ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 29\n",
    "\n",
    "Following the training of our random forest models, we decide to analyse the feature importance scores provided by the model built using 200 trees. Our aim is to identify which features the model considers most significant in predicting the target variable.\n",
    "\n",
    "Which of the following does the model consider to be the top 3 most significant features in predicting Standard_yield?\n",
    "\n",
    "#### Options\n",
    "\n",
    "* Soil_fertility, Rainfall, Slope\n",
    "* Rainfall, Crop_type_tea, Latitude\n",
    "* pH, Rainfall, Location_Rural_Hawassa\n",
    "* Elevation, Soil_fertility, pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "feature_importances_df=pd.DataFrame({'Features':X.columns,'Importance':feature_importances})\n",
    "\n",
    "feature_importances_df=feature_importances_df.sort_values(by='Importance',ascending=False)\n",
    "\n",
    "feature_importances_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 30\n",
    "Which of the following initialised random forest models will allow access and calculation of the Out-of-Bag (OOB) score for performance evaluation without requiring a separate validation set?\n",
    "\n",
    "#### Options\n",
    "* RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=2, random_state=42)\n",
    "* RandomForestRegressor(n_estimators=150, max_depth=None, oob_score=True)\n",
    "* RandomForestRegressor(n_estimators=200, max_depth=5, min_samples_leaf=4, oob_score=False)\n",
    "* RandomForestRegressor(n_estimators=120, max_features='sqrt', min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
